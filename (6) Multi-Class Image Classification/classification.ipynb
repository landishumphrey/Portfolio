{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93dc5d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from array import array\n",
    "import os\n",
    "from skimage import io\n",
    "import cv2\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a25d86bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_metrics(y_test, y_pred):\n",
    "    # Evaluation metrics for classification\n",
    "    confusion = confusion_matrix(y_test, y_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion)\n",
    "    accuracy = round(accuracy_score(y_test,y_pred),2)\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "    precision = round(precision_score(y_test, y_pred, average = 'weighted'),2)\n",
    "    print(\"Precision: \", precision)\n",
    "    recall = round(recall_score(y_test, y_pred, average = 'weighted'),2)\n",
    "    print(\"Recall: \", recall)\n",
    "    f1 = round(f1_score(y_test, y_pred, average = 'weighted'),2)\n",
    "    print(\"F1 score: \", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ebfe558",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_clf_and_predict(x,y, test_x, classifier):\n",
    "    clf = classifier()\n",
    "    clf.fit(x,y)\n",
    "    \n",
    "    y_pred = clf.predict(test_x)\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9171299a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_and_metrics(train_func, test_func, feat_ext_name, classifier):\n",
    "    train_root_folder = \"./dataset/train\"\n",
    "    test_root_folder = \"./dataset/test\"   \n",
    "    \n",
    "    if feat_ext_name == \"Sobel\":\n",
    "        train_data_x, train_data_y, train_data_xy, labels = train_func(train_root_folder)\n",
    "        \n",
    "        test_data_x, test_data_y, test_data_xy, filenames = test_func(test_root_folder)\n",
    "        \n",
    "        y_pred_x = fit_clf_and_predict(train_data_x, labels, test_data_x, classifier)\n",
    "        y_pred_y = fit_clf_and_predict(train_data_y, labels, test_data_y, classifier)\n",
    "        y_pred_xy = fit_clf_and_predict(train_data_xy, labels, test_data_xy, classifier)\n",
    "        \n",
    "        print(\"Metrics for\", feat_ext_name, \"Feature Extraction:\")\n",
    "        print(\"Detection on x-axis:\")\n",
    "        my_metrics(filenames, y_pred_x)\n",
    "        print(\"Detection on y-axis:\")\n",
    "        my_metrics(filenames, y_pred_y)\n",
    "        print(\"Detection on both axes:\")\n",
    "        my_metrics(filenames, y_pred_xy)\n",
    "        \n",
    "    else:\n",
    "        imgs, labels = train_func(train_root_folder)\n",
    "\n",
    "        t_imgs, t_labels = test_func(test_root_folder)\n",
    "\n",
    "        y_pred = fit_clf_and_predict(imgs, labels, t_imgs, classifier)\n",
    "        \n",
    "        print(\"Metrics for\", feat_ext_name, \"Feature Extraction:\")\n",
    "        my_metrics(t_labels, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb1d9d0",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545e7d99",
   "metadata": {},
   "source": [
    "## Gabor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "184d088e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "\n",
    "# Function to load images from folders and assign labels \n",
    "def train_images_to_array_gabor(root_folder):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for label in os.listdir(root_folder):\n",
    "        i = 0\n",
    "        label_folder = os.path.join(root_folder, label)\n",
    "        if os.path.isdir(label_folder):\n",
    "            for filename in os.listdir(label_folder):\n",
    "                img_path = os.path.join(label_folder, filename)\n",
    "                try:\n",
    "                    img = cv2.imread(img_path)\n",
    "                    if img is not None:\n",
    "                        print(i, end = '\\r')\n",
    "                        #print((len(os.listdir(root_folder)) - i), end='\\r') #Serves as an ad-hoc progress bar for img processing\n",
    "                        i += 1\n",
    "                        gabor_features = gabor_texture_analysis(img)\n",
    "                        images.append(gabor_features) \n",
    "                        labels.append(label)\n",
    "                except:\n",
    "                    continue\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "\n",
    "# Test data\n",
    "\n",
    "# Function to load images from folders and convert to NumPy array\n",
    "def test_images_to_array_gabor(root_folder):\n",
    "    data = []\n",
    "    filenames = []\n",
    "    i = 0\n",
    "    for filename in os.listdir(root_folder):\n",
    "        img_path = os.path.join(root_folder, filename)\n",
    "        try:\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is not None:\n",
    "                print(i, end = '\\r')\n",
    "                #print((len(os.listdir(root_folder)) - i), end='\\r') #Serves as an ad-hoc progress bar for img processing\n",
    "                i += 1\n",
    "                gabor_features = gabor_texture_analysis(img)\n",
    "                data.append(gabor_features) \n",
    "                filenames.append(filename.split()[0])\n",
    "        except:\n",
    "            continue\n",
    "    return np.array(data), filenames\n",
    "\n",
    "\n",
    "# Gabor Feature Extraction\n",
    "def gabor_texture_analysis(image, num_theta=8, num_lambda=5):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    features = []\n",
    "\n",
    "    # Define the range for theta (orientation) and lambda (wavelength)\n",
    "    thetas = np.linspace(0, np.pi, num_theta)\n",
    "    lambdas = np.linspace(0.1, 0.5, num_lambda)\n",
    "\n",
    "    # Loop over each theta and lambda\n",
    "    for theta in thetas:\n",
    "        for lambd in lambdas:\n",
    "            # Create Gabor kernel\n",
    "            kernel = cv2.getGaborKernel((21, 21), 5.0, theta, lambd, 1.0, 0, ktype=cv2.CV_32F)\n",
    "            # Apply the Gabor kernel to the image\n",
    "            filtered = cv2.filter2D(gray, cv2.CV_8UC3, kernel)\n",
    "            # Compute mean and standard deviation of the filtered image\n",
    "            mean, std_dev = cv2.meanStdDev(filtered)\n",
    "            # Append mean and standard deviation as features\n",
    "            features.extend([mean[0][0], std_dev[0][0]])\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a02e7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for Gabor Feature Extraction:\n",
      "Confusion Matrix:\n",
      "[[237   0  18  30  15]\n",
      " [  0 295   4   0   1]\n",
      " [ 15   3 264   4  14]\n",
      " [ 18   0   5 277   0]\n",
      " [ 17   1  23   1 258]]\n",
      "Accuracy:  0.89\n",
      "Precision:  0.89\n",
      "Recall:  0.89\n",
      "F1 score:  0.89\n"
     ]
    }
   ],
   "source": [
    "train_test_and_metrics(train_images_to_array_gabor, test_images_to_array_gabor, \"Gabor\", DecisionTreeClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7d2916",
   "metadata": {},
   "source": [
    "## Sobel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8bce449a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "\n",
    "# Function to load images from folders and assign labels\n",
    "def train_images_to_array_sobel(root_folder):\n",
    "    data_x = []\n",
    "    data_y = []\n",
    "    data_xy = []\n",
    "    labels = []\n",
    "    for label in os.listdir(root_folder):\n",
    "        i = 0\n",
    "        label_folder = os.path.join(root_folder, label)\n",
    "        if os.path.isdir(label_folder):\n",
    "            for filename in os.listdir(label_folder):\n",
    "                img_path = os.path.join(label_folder, filename)\n",
    "                try:\n",
    "                    img = cv2.imread(img_path)\n",
    "                    if img is not None:\n",
    "                        print(i, end = '\\r')\n",
    "                        #print((len(os.listdir(label_folder)) - i), end='\\r') #Serves as an ad-hoc progress bar for img processing\n",
    "                        i += 1\n",
    "                        img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "                        img_blur = cv2.GaussianBlur(img_gray, (3,3), 0)\n",
    "                        sobel_x, sobel_y, sobelxy = sobel_edge_detection(img_blur, 5)\n",
    "                        data_x.append(sobel_x.flatten())  # Flatten the image and add to the list\n",
    "                        data_y.append(sobel_y.flatten())  # Flatten the image and add to the list\n",
    "                        data_xy.append(sobelxy.flatten())  # Flatten the image and add to the list\n",
    "                        labels.append(label)  # Assign label from folder name\n",
    "                except:\n",
    "                    continue\n",
    "                    \n",
    "    return np.array(data_x), np.array(data_y), np.array(data_xy), np.array(labels)\n",
    "\n",
    "\n",
    "# Test data\n",
    "\n",
    "# Function to load images from folders and convert to NumPy array\n",
    "def test_images_to_array_sobel(root_folder):\n",
    "    data_x = []\n",
    "    data_y = []\n",
    "    data_xy = []\n",
    "    filenames = []\n",
    "    i = 0\n",
    "    for filename in os.listdir(root_folder):\n",
    "        img_path = os.path.join(root_folder, filename)\n",
    "        try:\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is not None:\n",
    "                print((len(os.listdir(root_folder)) - i), end='\\r') #Serves as an ad-hoc progress bar for img processing\n",
    "                i += 1\n",
    "                img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "                img_blur = cv2.GaussianBlur(img_gray, (3,3), 0)\n",
    "                sobel_x, sobel_y, sobelxy = sobel_edge_detection(img_blur, 5)\n",
    "                data_x.append(sobel_x.flatten())  # Flatten the image and add to the list\n",
    "                data_y.append(sobel_y.flatten())  # Flatten the image and add to the list\n",
    "                data_xy.append(sobelxy.flatten())  # Flatten the image and add to the list\n",
    "                filenames.append(filename.split()[0])\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    return np.array(data_x), np.array(data_y), np.array(data_xy), filenames\n",
    "\n",
    "\n",
    "# Sobel Edge Detection\n",
    "def sobel_edge_detection(image_blur, k_size):\n",
    "    sobelx = cv2.Sobel(src=image_blur, ddepth=cv2.CV_64F, dx=1, dy=0, ksize= k_size)\n",
    "    sobely = cv2.Sobel(src=image_blur, ddepth=cv2.CV_64F, dx=0, dy=1, ksize= k_size)\n",
    "    sobelxy = cv2.Sobel(src=image_blur, ddepth=cv2.CV_64F, dx=1,dy=1, ksize= k_size)\n",
    "    \n",
    "    return sobelx, sobely, sobelxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a5d9b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for Sobel Feature Extraction:\n",
      "Detection on x-axis:\n",
      "Confusion Matrix:\n",
      "[[206   9  23  58   4]\n",
      " [  4 267  12   4  13]\n",
      " [ 20   8 235   6  31]\n",
      " [ 21   4   3 270   2]\n",
      " [  3   5  38   0 254]]\n",
      "Accuracy:  0.82\n",
      "Precision:  0.82\n",
      "Recall:  0.82\n",
      "F1 score:  0.82\n",
      "Detection on y-axis:\n",
      "Confusion Matrix:\n",
      "[[225   6  18  48   3]\n",
      " [  6 265  14   4  11]\n",
      " [ 19  11 230   3  37]\n",
      " [ 46   1   5 246   2]\n",
      " [  5   9  51   2 233]]\n",
      "Accuracy:  0.8\n",
      "Precision:  0.8\n",
      "Recall:  0.8\n",
      "F1 score:  0.8\n",
      "Detection on both axes:\n",
      "Confusion Matrix:\n",
      "[[225   4  32  33   6]\n",
      " [ 10 253  15   2  20]\n",
      " [ 32   7 208   7  46]\n",
      " [ 38   0   3 257   2]\n",
      " [ 10  15  49   1 225]]\n",
      "Accuracy:  0.78\n",
      "Precision:  0.78\n",
      "Recall:  0.78\n",
      "F1 score:  0.78\n"
     ]
    }
   ],
   "source": [
    "train_test_and_metrics(train_images_to_array_sobel, test_images_to_array_sobel, \"Sobel\", DecisionTreeClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f7fe71",
   "metadata": {},
   "source": [
    "## Canny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a05cbe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "\n",
    "# Function to load images from folders and assign labels \n",
    "def train_images_to_array_canny(root_folder):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for label in os.listdir(root_folder):\n",
    "        i = 0\n",
    "        label_folder = os.path.join(root_folder, label)\n",
    "        if os.path.isdir(label_folder):\n",
    "            for filename in os.listdir(label_folder):\n",
    "                img_path = os.path.join(label_folder, filename)\n",
    "                try:\n",
    "                    img = cv2.imread(img_path)\n",
    "                    if img is not None:\n",
    "                        print((len(os.listdir(label_folder)) - i), end='\\r') #Serves as an ad-hoc progress bar for img processing\n",
    "                        i += 1\n",
    "                        img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "                        img_blur = cv2.GaussianBlur(img_gray, (3,3), 0)\n",
    "                        edges = cv2.Canny(image=img_blur, threshold1=100, threshold2=200)\n",
    "                        images.append(edges.flatten())  # Flatten the image and add to the list\n",
    "                        labels.append(label)  # Assign label from folder name\n",
    "                except:\n",
    "                    continue\n",
    "                    \n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "\n",
    "# Test data\n",
    "\n",
    "# Function to load images from folders and convert to NumPy array\n",
    "def test_images_to_array_canny(root_folder):\n",
    "    data = []\n",
    "    filenames = []\n",
    "    i = 0\n",
    "    for filename in os.listdir(root_folder):\n",
    "        img_path = os.path.join(root_folder, filename)\n",
    "        try:\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is not None:\n",
    "                print((len(os.listdir(root_folder)) - i), end='\\r') #Serves as an ad-hoc progress bar for img processing\n",
    "                i += 1\n",
    "                img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "                img_blur = cv2.GaussianBlur(img_gray, (3,3), 0)\n",
    "                edges = cv2.Canny(image=img_blur, threshold1=100, threshold2=200)\n",
    "                data.append(edges.flatten())  # Append hog image\n",
    "                filenames.append(filename.split()[0])\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    return np.array(data), filenames\n",
    "\n",
    "\n",
    "# Canny Edge Detection\n",
    "def canny_edge_detection(image_blur, threshold_1, threshold_2):\n",
    "    edges = cv2.Canny(image=img_blur, threshold1=threshold_1, threshold2=threshold_2)\n",
    "    \n",
    "    return edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51542143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for Canny Feature Extraction:\n",
      "Confusion Matrix:\n",
      "[[147  14  29  99  11]\n",
      " [ 14 248   1  34   3]\n",
      " [ 39  13 133  20  95]\n",
      " [ 94  21   5 178   2]\n",
      " [ 23   5 110  10 152]]\n",
      "Accuracy:  0.57\n",
      "Precision:  0.57\n",
      "Recall:  0.57\n",
      "F1 score:  0.57\n"
     ]
    }
   ],
   "source": [
    "train_test_and_metrics(train_images_to_array_canny, test_images_to_array_canny, \"Canny\", DecisionTreeClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15e5228",
   "metadata": {},
   "source": [
    "## Hog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e9afb2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import hog\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Training data\n",
    "\n",
    "# Function to load images from folders, assign labels, and convert to HOG\n",
    "def train_images_to_array_hog(root_folder):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for label in os.listdir(root_folder):\n",
    "        i = 0\n",
    "        print(label)\n",
    "        label_folder = os.path.join(root_folder, label)\n",
    "        if os.path.isdir(label_folder):\n",
    "            for filename in os.listdir(label_folder):\n",
    "                img_path = os.path.join(label_folder, filename)\n",
    "                try:\n",
    "                    img = Image.open(img_path).convert(\"L\")  # Convert image to grayscale\n",
    "                    if img is not None:\n",
    "                        print((len(os.listdir(label_folder)) - i), end='\\r') #Serves as ad-hoc progress bar for img processing\n",
    "                        i += 1\n",
    "                        img_array = np.array(img)\n",
    "                        img = hog(img_array)\n",
    "                        images.append(img)  # Flatten the HOG data and add to the list\n",
    "                        labels.append(label)  # Assign label from folder name\n",
    "                except:\n",
    "                    continue\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "\n",
    "# Test data\n",
    "\n",
    "# Function to load images from folders and convert to HOG\n",
    "def test_images_to_array_hog(root_folder):\n",
    "    data = []\n",
    "    filenames = []\n",
    "    i = 0\n",
    "    for filename in os.listdir(root_folder):\n",
    "        img_path = os.path.join(root_folder, filename)\n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"L\")  # Convert image to grayscale\n",
    "            if img is not None:\n",
    "                print((len(os.listdir(root_folder)) - i), end='\\r') #Serves as an ad-hoc progress bar for img processing\n",
    "                i += 1\n",
    "                img_array = np.array(img)\n",
    "                img = hog(img_array)\n",
    "                data.append(img)  # Append HOG data\n",
    "                filenames.append(filename.split()[0])\n",
    "        except:\n",
    "            continue\n",
    "    return np.array(data), filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dc758ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".DS_Store\n",
      "Arborio\n",
      "basmati\n",
      "Ipsala\n",
      "Jasmine\n",
      "Karacadag\n",
      "Metrics for Hog Feature Extraction:\n",
      "Confusion Matrix:\n",
      "[[258   2  16  23   1]\n",
      " [  4 291   0   0   5]\n",
      " [ 10   4 265   0  21]\n",
      " [ 19   0   0 280   1]\n",
      " [  2   0  23   0 275]]\n",
      "Accuracy:  0.91\n",
      "Precision:  0.91\n",
      "Recall:  0.91\n",
      "F1 score:  0.91\n"
     ]
    }
   ],
   "source": [
    "train_test_and_metrics(train_images_to_array_hog, test_images_to_array_hog, \"Hog\", DecisionTreeClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e129da1",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9a9aff63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import splitfolders\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory # type: ignore\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt # type: ignore\n",
    "import seaborn as sns # type: ignore\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential # type: ignore\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout # type: ignore\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "73661211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 42000 files belonging to 5 classes.\n",
      "Found 6000 files belonging to 5 classes.\n",
      "Found 12000 files belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "train_dataset = image_dataset_from_directory(\n",
    " \"ricedata/train\",\n",
    "    labels = 'inferred',\n",
    "    image_size=(180, 180),\n",
    "    batch_size=32)\n",
    "test_dataset = image_dataset_from_directory(\n",
    "   \"ricedata/test\",\n",
    "    labels = 'inferred',\n",
    "    image_size=(180, 180),\n",
    "    batch_size=32)\n",
    "validation_dataset = image_dataset_from_directory(\n",
    "   \"ricedata/val\",\n",
    "    labels = 'inferred',\n",
    "    image_size=(180, 180),\n",
    "    batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e701e084",
   "metadata": {},
   "source": [
    "## Gabor Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3e8179a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gabor\n",
    "def build_gabor_kernel(ksize, sigma, theta, lambd, gamma, psi):\n",
    "    # Define the size of the filter\n",
    "    axis = tf.range(-ksize // 2 + 1, ksize // 2 + 1, dtype=tf.float32)\n",
    "    x, y = tf.meshgrid(axis, axis)\n",
    "\n",
    "    # Rotation parameters\n",
    "    x_theta = x * tf.math.cos(theta) + y * tf.math.sin(theta)\n",
    "    y_theta = -x * tf.math.sin(theta) + y * tf.math.cos(theta)\n",
    "\n",
    "    gb = tf.exp(-0.5 * (x_theta**2 + gamma**2 * y_theta**2) / sigma**2) * tf.cos(2. * np.pi * x_theta / lambd + psi)\n",
    "    gb = tf.expand_dims(gb, -1)  # For grayscale\n",
    "    gb = tf.expand_dims(gb, -1)  # For filter\n",
    "    return gb\n",
    "\n",
    "def apply_gabor_filter(image, ksize=21, sigma=8, theta=1.57, lambd=10, gamma=0.5, psi=0):\n",
    "    kernel = build_gabor_kernel(ksize, sigma, theta, lambd, gamma, psi)\n",
    "    # Convert image to grayscale and add batch and channel dimensions\n",
    "    image = tf.image.rgb_to_grayscale(image)\n",
    "    filtered_image = tf.nn.conv2d(image, kernel, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "    return filtered_image\n",
    "\n",
    "def preprocess_image(image, label):\n",
    "    # Apply the Gabor filter\n",
    "    filtered_image = apply_gabor_filter(image)\n",
    "    return filtered_image, label\n",
    "\n",
    "train_dataset_gabor = train_dataset.map(preprocess_image)\n",
    "validation_dataset_gabor = validation_dataset.map(preprocess_image)\n",
    "test_dataset_gabor = test_dataset.map(preprocess_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcfea21",
   "metadata": {},
   "source": [
    "## HOG Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696827ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOG\n",
    "def approximate_hog(image):\n",
    "    # Convert to grayscale\n",
    "    image = tf.image.rgb_to_grayscale(image)\n",
    "    # Calculate gradients\n",
    "    dx, dy = tf.image.image_gradients(image)\n",
    "    # Compute gradient magnitude and angle\n",
    "    magnitude = tf.sqrt(tf.square(dx) + tf.square(dy))\n",
    "    angle = tf.atan2(dy, dx)\n",
    "    # Further processing can be done to mimic histogramming of gradients in cells as done in HOG\n",
    "    return magnitude  # Simplified output for this example\n",
    "\n",
    "def preprocess_image(image, label):\n",
    "    # Apply approximate HOG\n",
    "    image = approximate_hog(image)\n",
    "    return image, label\n",
    "\n",
    "# Update your dataset processing\n",
    "train_dataset_hog = train_dataset.map(preprocess_image)\n",
    "validation_dataset_hog = validation_dataset.map(preprocess_image)\n",
    "test_dataset_hog = test_dataset.map(preprocess_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca43b83",
   "metadata": {},
   "source": [
    "## Sobel Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298a515a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sobel\n",
    "def apply_sobel(image):\n",
    "    # Sobel filter kernels for edge detection in x and y directions\n",
    "    sobel_x = tf.constant([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=tf.float32)\n",
    "    sobel_y = tf.constant([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=tf.float32)\n",
    "    sobel_x_filter = tf.reshape(sobel_x, [3, 3, 1, 1])\n",
    "    sobel_y_filter = tf.reshape(sobel_y, [3, 3, 1, 1])\n",
    "\n",
    "    # Convert image to grayscale\n",
    "    gray = tf.image.rgb_to_grayscale(image)\n",
    "\n",
    "    # Expand dimensions to apply the filter\n",
    "    gray = tf.expand_dims(gray, axis=0)\n",
    "\n",
    "    # Apply Sobel filtering\n",
    "    filtered_x = tf.nn.conv2d(input=gray, filters=sobel_x_filter, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    filtered_y = tf.nn.conv2d(input=gray, filters=sobel_y_filter, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "    # Calculate the magnitude of the gradients\n",
    "    sobel_output = tf.sqrt(tf.square(filtered_x) + tf.square(filtered_y))\n",
    "\n",
    "    # Remove dimensions of size 1 from the shape of the tensor\n",
    "    sobel_output = tf.squeeze(sobel_output, axis=0)\n",
    "\n",
    "    return sobel_output\n",
    "\n",
    "def preprocess_image(image, label):\n",
    "    # Apply the Sobel filter\n",
    "    image = apply_sobel(image)\n",
    "    return image, label\n",
    "\n",
    "train_dataset_sobel = train_dataset.map(preprocess_image)\n",
    "validation_dataset_sobel = validation_dataset.map(preprocess_image)\n",
    "test_dataset_sobel = test_dataset.map(preprocess_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d20085",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d763770d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "\n",
    "def build_model():\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=(180, 180, 1)),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(5, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb1099e",
   "metadata": {},
   "source": [
    "## Gabor Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "118b4f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m466s\u001b[0m 353ms/step - accuracy: 0.8626 - loss: 16.9477 - val_accuracy: 0.9592 - val_loss: 0.1148\n",
      "Epoch 2/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m460s\u001b[0m 350ms/step - accuracy: 0.9519 - loss: 0.1452 - val_accuracy: 0.9694 - val_loss: 0.0905\n",
      "Epoch 3/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m524s\u001b[0m 399ms/step - accuracy: 0.9605 - loss: 0.1252 - val_accuracy: 0.9687 - val_loss: 0.1034\n",
      "Epoch 4/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m460s\u001b[0m 350ms/step - accuracy: 0.9613 - loss: 0.1227 - val_accuracy: 0.9721 - val_loss: 0.0882\n",
      "Epoch 5/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m463s\u001b[0m 352ms/step - accuracy: 0.9683 - loss: 0.0998 - val_accuracy: 0.9716 - val_loss: 0.0851\n",
      "Epoch 6/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m461s\u001b[0m 350ms/step - accuracy: 0.9674 - loss: 0.0959 - val_accuracy: 0.9708 - val_loss: 0.0850\n",
      "Epoch 7/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m462s\u001b[0m 351ms/step - accuracy: 0.9667 - loss: 0.1031 - val_accuracy: 0.9722 - val_loss: 0.0825\n",
      "Epoch 8/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m469s\u001b[0m 357ms/step - accuracy: 0.9691 - loss: 0.0886 - val_accuracy: 0.9722 - val_loss: 0.0845\n",
      "Epoch 9/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m475s\u001b[0m 361ms/step - accuracy: 0.9721 - loss: 0.0838 - val_accuracy: 0.9705 - val_loss: 0.0878\n",
      "Epoch 10/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m490s\u001b[0m 373ms/step - accuracy: 0.9731 - loss: 0.0798 - val_accuracy: 0.9712 - val_loss: 0.1106\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x235bf696350>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gabor\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(train_dataset_gabor, epochs=1, validation_data=validation_dataset_gabor)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(test_dataset_gabor)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# Confusion matrix and additional metrics\n",
    "true_labels = []\n",
    "predictions = []\n",
    "\n",
    "for images, labels in test_dataset_gabor:\n",
    "    pred = model.predict(images)  # Use the preprocessed test dataset\n",
    "    true_labels.extend(labels.numpy())\n",
    "    predictions.extend(np.argmax(pred, axis=1))\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "precision = precision_score(true_labels, predictions, average='macro')\n",
    "recall = recall_score(true_labels, predictions, average='macro')\n",
    "f1 = f1_score(true_labels, predictions, average='macro')\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-Score: {f1:.2f}\")\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b898e631",
   "metadata": {},
   "source": [
    "## HOG Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d4b56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOG\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(train_dataset_hog, epochs=10, validation_data=validation_dataset_hog)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(test_dataset_hog)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# Confusion matrix and additional metrics\n",
    "true_labels = []\n",
    "predictions = []\n",
    "\n",
    "for images, labels in test_dataset_hog:\n",
    "    pred = model.predict(images)  # Use the preprocessed test dataset\n",
    "    true_labels.extend(labels.numpy())\n",
    "    predictions.extend(np.argmax(pred, axis=1))\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "precision = precision_score(true_labels, predictions, average='macro')\n",
    "recall = recall_score(true_labels, predictions, average='macro')\n",
    "f1 = f1_score(true_labels, predictions, average='macro')\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-Score: {f1:.2f}\")\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7ff813",
   "metadata": {},
   "source": [
    "## Sobel Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b020a175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sobel\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(train_dataset_sobel, epochs=10, validation_data=validation_dataset_sobel)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(test_dataset_sobel)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# Confusion matrix and additional metrics\n",
    "true_labels = []\n",
    "predictions = []\n",
    "\n",
    "for images, labels in test_dataset_sobel:\n",
    "    pred = model.predict(images)  # Use the preprocessed test dataset\n",
    "    true_labels.extend(labels.numpy())\n",
    "    predictions.extend(np.argmax(pred, axis=1))\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "precision = precision_score(true_labels, predictions, average='macro')\n",
    "recall = recall_score(true_labels, predictions, average='macro')\n",
    "f1 = f1_score(true_labels, predictions, average='macro')\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-Score: {f1:.2f}\")\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f0ca6b",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d51e325",
   "metadata": {},
   "source": [
    "## Gabor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "627178ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for Gabor Feature Extraction:\n",
      "Confusion Matrix:\n",
      "[[276   0  13   9   2]\n",
      " [  1 295   4   0   0]\n",
      " [  2   0 290   0   8]\n",
      " [  5   0   1 294   0]\n",
      " [  0   0  11   0 289]]\n",
      "Accuracy:  0.96\n",
      "Precision:  0.96\n",
      "Recall:  0.96\n",
      "F1 score:  0.96\n"
     ]
    }
   ],
   "source": [
    "train_test_and_metrics(train_images_to_array_gabor, test_images_to_array_gabor, \"Gabor\", RandomForestClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be402ae0",
   "metadata": {},
   "source": [
    "## Sobel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "65b9bd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for Sobel Feature Extraction:\n",
      "Detection on x-axis:\n",
      "Confusion Matrix:\n",
      "[[281   0   5  14   0]\n",
      " [  1 299   0   0   0]\n",
      " [  1   2 281   0  16]\n",
      " [ 10   0   0 290   0]\n",
      " [  1   0   7   0 292]]\n",
      "Accuracy:  0.96\n",
      "Precision:  0.96\n",
      "Recall:  0.96\n",
      "F1 score:  0.96\n",
      "Detection on y-axis:\n",
      "Confusion Matrix:\n",
      "[[280   0   4  16   0]\n",
      " [  1 299   0   0   0]\n",
      " [  0   8 272   0  20]\n",
      " [ 10   0   0 290   0]\n",
      " [  0   0   8   0 292]]\n",
      "Accuracy:  0.96\n",
      "Precision:  0.96\n",
      "Recall:  0.96\n",
      "F1 score:  0.96\n",
      "Detection on both axes:\n",
      "Confusion Matrix:\n",
      "[[283   0   4  13   0]\n",
      " [  1 299   0   0   0]\n",
      " [  1   9 251   0  39]\n",
      " [ 12   0   0 288   0]\n",
      " [  1   0   6   0 293]]\n",
      "Accuracy:  0.94\n",
      "Precision:  0.94\n",
      "Recall:  0.94\n",
      "F1 score:  0.94\n"
     ]
    }
   ],
   "source": [
    "train_test_and_metrics(train_images_to_array_sobel, test_images_to_array_sobel, \"Sobel\", RandomForestClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64aa3db4",
   "metadata": {},
   "source": [
    "## Canny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "736b7287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for Canny Feature Extraction:\n",
      "Confusion Matrix:\n",
      "[[202   2   7  89   0]\n",
      " [ 14 266   1  19   0]\n",
      " [ 25   0 195   1  79]\n",
      " [ 41   8   0 251   0]\n",
      " [  3   0 111   0 186]]\n",
      "Accuracy:  0.73\n",
      "Precision:  0.74\n",
      "Recall:  0.73\n",
      "F1 score:  0.73\n"
     ]
    }
   ],
   "source": [
    "train_test_and_metrics(train_images_to_array_canny, test_images_to_array_canny, \"Canny\", RandomForestClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e3419c",
   "metadata": {},
   "source": [
    "## HOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7cec45f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".DS_Store\n",
      "Arborio\n",
      "basmati\n",
      "Ipsala\n",
      "Jasmine\n",
      "Karacadag\n",
      "Metrics for Hog Feature Extraction:\n",
      "Confusion Matrix:\n",
      "[[277   0   7  16   0]\n",
      " [  1 299   0   0   0]\n",
      " [  0   3 285   0  12]\n",
      " [ 13   0   0 287   0]\n",
      " [  0   0   5   0 295]]\n",
      "Accuracy:  0.96\n",
      "Precision:  0.96\n",
      "Recall:  0.96\n",
      "F1 score:  0.96\n"
     ]
    }
   ],
   "source": [
    "train_test_and_metrics(train_images_to_array_hog, test_images_to_array_hog, \"Hog\", RandomForestClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c398755b",
   "metadata": {},
   "source": [
    "# Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2775cfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_metrics(train_func, test_func):\n",
    "    images, labels = train_func(\"./dataset/train\")\n",
    "    t_imgs, t_labels = test_func(\"./dataset/test\")\n",
    "    \n",
    "    x_train = images\n",
    "    y_train = labels\n",
    "    \n",
    "    x_test = t_imgs\n",
    "    y_test = t_labels\n",
    "    \n",
    "    y_pred_poly_full = svm_algorithm(x_train, y_train, x_test, 'poly', 5 )\n",
    "    print('Full Dataset SVM Kernel= Poly')\n",
    "    my_metrics(y_test, y_pred_poly_full)\n",
    "\n",
    "    y_pred_rbf_full = svm_algorithm(x_train, y_train, x_test, 'rbf', 5 )\n",
    "    print('Full Dataset SVM Kernel= Radias Bias Function')\n",
    "    my_metrics(y_test, y_pred_rbf_full)\n",
    "\n",
    "    # Kernel is Linear\n",
    "    y_pred_linear_full = svm_algorithm(x_train, y_train, x_test, 'linear', 5)\n",
    "    print('Full Dataset SVM Kernel= Linear')\n",
    "    my_metrics(y_test, y_pred_linear_full)\n",
    "\n",
    "    y_pred_sig_full = svm_algorithm(x_train, y_train, x_test, 'sigmoid', 5 )\n",
    "    print('Full Dataset SVM Kernel= Sigmoid')\n",
    "    my_metrics(y_test, y_pred_sig_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3118452c",
   "metadata": {},
   "source": [
    "## Gabor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "429a10b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c8cd9769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_algorithm(x_train, y_train, x_test, kernel_type, c_value):\n",
    "    # Create SVM classifier\n",
    "    svm_classifier = svm.SVC(kernel=kernel_type, C=c_value)\n",
    "\n",
    "    # Train the classifier\n",
    "    svm_classifier.fit(x_train, y_train)\n",
    "\n",
    "    # Predict on the test data\n",
    "    y_pred = svm_classifier.predict(x_test)\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5c15cc08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1499\r"
     ]
    }
   ],
   "source": [
    "images, labels = train_images_to_array_gabor(\"./dataset/train\")\n",
    "t_imgs, t_labels = test_images_to_array_gabor(\"./dataset/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "19f62190",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = images\n",
    "y_train = labels\n",
    "x_test = t_imgs\n",
    "y_test = t_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "47039498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Dataset SVM Kernel= Poly\n",
      "Confusion Matrix:\n",
      "[[272   0  15   9   4]\n",
      " [  1 299   0   0   0]\n",
      " [  6  13 242   1  38]\n",
      " [  5   0   3 292   0]\n",
      " [ 11   1  20   0 268]]\n",
      "Accuracy:  0.92\n",
      "Precision:  0.91\n",
      "Recall:  0.92\n",
      "F1 score:  0.91\n",
      "Full Dataset SVM Kernel= Radias Bias Function\n",
      "Confusion Matrix:\n",
      "[[270   0  14  10   6]\n",
      " [  1 299   0   0   0]\n",
      " [  6  13 234   1  46]\n",
      " [  6   0   2 292   0]\n",
      " [  9   1  19   0 271]]\n",
      "Accuracy:  0.91\n",
      "Precision:  0.91\n",
      "Recall:  0.91\n",
      "F1 score:  0.91\n",
      "Full Dataset SVM Kernel= Linear\n",
      "Confusion Matrix:\n",
      "[[294   0   4   2   0]\n",
      " [  1 298   1   0   0]\n",
      " [  6   3 282   0   9]\n",
      " [  4   0   0 296   0]\n",
      " [  4   0  23   0 273]]\n",
      "Accuracy:  0.96\n",
      "Precision:  0.96\n",
      "Recall:  0.96\n",
      "F1 score:  0.96\n",
      "Full Dataset SVM Kernel= Sigmoid\n",
      "Confusion Matrix:\n",
      "[[  0 300   0   0   0]\n",
      " [  0   2 298   0   0]\n",
      " [  0 274  26   0   0]\n",
      " [  0 300   0   0   0]\n",
      " [  0 287  13   0   0]]\n",
      "Accuracy:  0.02\n",
      "Precision:  0.02\n",
      "Recall:  0.02\n",
      "F1 score:  0.02\n"
     ]
    }
   ],
   "source": [
    "y_pred_poly_full = svm_algorithm(x_train, y_train, x_test, 'poly', 5 )\n",
    "print('Full Dataset SVM Kernel= Poly')\n",
    "my_metrics(y_test, y_pred_poly_full)\n",
    "\n",
    "y_pred_rbf_full = svm_algorithm(x_train, y_train, x_test, 'rbf', 5 )\n",
    "print('Full Dataset SVM Kernel= Radias Bias Function')\n",
    "my_metrics(y_test, y_pred_rbf_full)\n",
    "\n",
    "# Kernel is Linear\n",
    "y_pred_linear_full = svm_algorithm(x_train, y_train, x_test, 'linear', 5)\n",
    "print('Full Dataset SVM Kernel= Linear')\n",
    "my_metrics(y_test, y_pred_linear_full)\n",
    "\n",
    "y_pred_sig_full = svm_algorithm(x_train, y_train, x_test, 'sigmoid', 5 )\n",
    "print('Full Dataset SVM Kernel= Sigmoid')\n",
    "my_metrics(y_test, y_pred_sig_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a455239",
   "metadata": {},
   "source": [
    "## HOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "09c0ccad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".DS_Store\n",
      "Arborio\n",
      "basmati\n",
      "Ipsala\n",
      "Jasmine\n",
      "Karacadag\n",
      "Full Dataset SVM Kernel= Poly\n",
      "Confusion Matrix:\n",
      "[[290   0   1   9   0]\n",
      " [  1 297   2   0   0]\n",
      " [  0   0 297   0   3]\n",
      " [  4   0   0 296   0]\n",
      " [  0   0   6   0 294]]\n",
      "Accuracy:  0.98\n",
      "Precision:  0.98\n",
      "Recall:  0.98\n",
      "F1 score:  0.98\n",
      "Full Dataset SVM Kernel= Radias Bias Function\n",
      "Confusion Matrix:\n",
      "[[290   0   1   9   0]\n",
      " [  1 297   2   0   0]\n",
      " [  0   0 296   0   4]\n",
      " [  4   0   0 296   0]\n",
      " [  0   0   6   0 294]]\n",
      "Accuracy:  0.98\n",
      "Precision:  0.98\n",
      "Recall:  0.98\n",
      "F1 score:  0.98\n",
      "Full Dataset SVM Kernel= Linear\n",
      "Confusion Matrix:\n",
      "[[289   0   1  10   0]\n",
      " [  2 298   0   0   0]\n",
      " [  0   0 293   0   7]\n",
      " [  4   0   0 296   0]\n",
      " [  0   0   6   0 294]]\n",
      "Accuracy:  0.98\n",
      "Precision:  0.98\n",
      "Recall:  0.98\n",
      "F1 score:  0.98\n",
      "Full Dataset SVM Kernel= Sigmoid\n",
      "Confusion Matrix:\n",
      "[[284   0   2  14   0]\n",
      " [  2 298   0   0   0]\n",
      " [  0   0 293   0   7]\n",
      " [  4   0   0 296   0]\n",
      " [  0   0   6   0 294]]\n",
      "Accuracy:  0.98\n",
      "Precision:  0.98\n",
      "Recall:  0.98\n",
      "F1 score:  0.98\n"
     ]
    }
   ],
   "source": [
    "svm_metrics(train_images_to_array_hog, test_images_to_array_hog)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403d357f",
   "metadata": {},
   "source": [
    "## Canny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6cdbc78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Dataset SVM Kernel= Poly\n",
      "Confusion Matrix:\n",
      "[[130   0   0 170   0]\n",
      " [  0   0   0 300   0]\n",
      " [  0   0 249  51   0]\n",
      " [  0   0   0 300   0]\n",
      " [  0   0 208  92   0]]\n",
      "Accuracy:  0.45\n",
      "Precision:  0.37\n",
      "Recall:  0.45\n",
      "F1 score:  0.35\n",
      "Full Dataset SVM Kernel= Radias Bias Function\n",
      "Confusion Matrix:\n",
      "[[278   0   7  15   0]\n",
      " [  2 298   0   0   0]\n",
      " [  0   3 256   0  41]\n",
      " [  7   0   0 293   0]\n",
      " [  0   0  13   0 287]]\n",
      "Accuracy:  0.94\n",
      "Precision:  0.94\n",
      "Recall:  0.94\n",
      "F1 score:  0.94\n",
      "Full Dataset SVM Kernel= Linear\n",
      "Confusion Matrix:\n",
      "[[276   0   3  21   0]\n",
      " [  7 293   0   0   0]\n",
      " [  3   2 273   0  22]\n",
      " [ 12   0   0 288   0]\n",
      " [  0   0  16   0 284]]\n",
      "Accuracy:  0.94\n",
      "Precision:  0.94\n",
      "Recall:  0.94\n",
      "F1 score:  0.94\n",
      "Full Dataset SVM Kernel= Sigmoid\n",
      "Confusion Matrix:\n",
      "[[271   1   4  24   0]\n",
      " [  6 294   0   0   0]\n",
      " [  2   2 273   0  23]\n",
      " [ 15   0   0 285   0]\n",
      " [  0   0  22   0 278]]\n",
      "Accuracy:  0.93\n",
      "Precision:  0.93\n",
      "Recall:  0.93\n",
      "F1 score:  0.93\n"
     ]
    }
   ],
   "source": [
    "svm_metrics(train_images_to_array_canny, test_images_to_array_canny)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce80d3fd",
   "metadata": {},
   "source": [
    "## Sobel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "672a3048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\r"
     ]
    }
   ],
   "source": [
    "train_data_x, train_data_y, train_data_xy, labels_sobel = train_images_to_array_sobel(\"./dataset/train\")\n",
    "\n",
    "test_data_x, test_data_y, test_data_xy, filenames_sobel = test_images_to_array_sobel(\"./dataset/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2d5ddfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_sobel_x = np.array(train_data_x)\n",
    "x_train_sobel_y = np.array(train_data_y)\n",
    "x_train_sobel_xy = np.array(train_data_xy)\n",
    "y_train = np.array(labels_sobel)\n",
    "\n",
    "y_test = np.array(filenames_sobel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab0192c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_poly_sobel = svm_algorithm(x_train_sobel_x, y_train, test_data_x, 'poly', 5)\n",
    "print('Dataset SVM Kernel = Poly ')\n",
    "my_metrics(y_test, y_pred_poly_sobel)\n",
    "\n",
    "y_pred_poly_sobel = svm_algorithm(x_train_sobel_x, y_train, test_data_x, 'Linear', 5)\n",
    "print('Dataset SVM Kernel = Linear ')\n",
    "my_metrics(y_test, y_pred_poly_sobel)\n",
    "\n",
    "y_pred_poly_sobel = svm_algorithm(x_train_sobel_x, y_train, test_data_x, 'rbf', 5)\n",
    "print('Dataset SVM Kernel = Radial Bias Function ')\n",
    "my_metrics(y_test, y_pred_poly_sobel)\n",
    "\n",
    "y_pred_poly_sobel = svm_algorithm(x_train_sobel_x, y_train, test_data_x, 'sigmoid', 5)\n",
    "print('Dataset SVM Kernel = sigmoid ')\n",
    "my_metrics(y_test, y_pred_poly_sobel)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
