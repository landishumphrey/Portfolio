# Portfolio
This repository serves as a portfolio for Landis Humphrey's projects, showcasing his abilities as a Data Analyst. The following projects were done over the course of his time at Champlain College. The details for each project are as follows:

## 1: Implementation Of Enhanced Stratified Sampling (ESS) Algorithm
Landis worked closely with one of his professors, Vikas Thammanna Gowda, assisting him in writing a research paper for submission in an academic journal. The topic of this paper closely followed that of the previous paper, where the Enhanced Stratified Sampling algorithm was first proposed in Gowda's publication in the 2024 IEEE International Conference on Future Machine Learning and Data Science (FMLDS). ESS is a method for use in Privacy-Preserving Big Data Publishing (PPDP), where data often involves sensitive information that must be privatized for the safety of individuals while still providing information for data analysis, a tradeoff between privacy and information loss. ESS automatically identifies the most effective size and diversity within equivalence classes, producing a dataset that has the best possible anonymity and utility. Landis implemented the algorithm in this project using the Python programming language and Jupyter Notebook environment. He used his acquired understanding to continue providing assistance to Professor Gowda for his 2025 research paper.

## 2: Diverse Facial Recognition With YOLO Algorithm
A common problem faced in object detection, especially facial recognition, is built-in bias inherited through the designer/trainer of the model. Landis investigated whether it was in fact possible to create an unbiased facial recognition model that properly identifies all colors, ages, and sizes of faces. He used the You Only Look Once (YOLO) algorithm as the foundation for this project. The model was trained on a dataset of diverse faces, containing varying combinations of features. After training, the model was more equal in its facial recognition, though there were still errors in detections, mostly in images with many faces or extremely dark skin. Landis used Python and Jupyter Notebook for this project.

## 3: Web Database Integration
As a part of Landis' Database Management Systems course, he created a simple database that a user could access and manipulate on the web. It somewhat mimics that of a school database, with information about students, courses, classrooms, and grades. Information can be added, edited, and deleted, but only if the user logs in with the ADMIN account information. The database is normalized and some tables are connected to others via foreign keys. This means that when an edit or deletion is made in one table, all connected tables containing that updated information will also be updated. There is a demo video where Landis and his partner, Thomas, walkthrough and illustrate the application's functionality. Landis used the React library to create the web user interface and SQLite for the featured integrated database (with MySQL used for earlier stages of the project).

## 4: Web Data Visualization Project: True vs. Biased
Data visualization is a powerful tool to highlight data and the insights that are extracted after analysis. Visualizations can be used to send a message and sometimes, this is used dishonestly. Landis and his partner, Aiden, explored this concept, creating pairs of visualizations that were based on the same data sample but portrayed different messaging: one truthful, one biased. Many changes made to the biased visualizations were small, but made a big difference in how the data looked and, consequently, what a viewer might think after viewing them. There is a showcase video where Landis and Aiden talk through what each pair of visualizations is based upon and trying to convey but also how their messaging differs. The visualizations were made using R, and were embedded into an HTML file, created using Go and Svelte.

## 5: S&P 500 Time Series Predictive Analysis
Data that exists as a time series can be used to predict future trends within itself. This is a common technique used in the stock market for making forecasts for how a stock(s) could perform. This is the foundation behind this project; Landis and Aiden conducted basic predictive analysis on the S&P 500, using data dating back to 1992. Several forecasting techniques were used, including Mean, Naive, and numerous Exponential Smoothing variations. The highest performing predictor was the Triple Exponential Smoothing (tES) model, which displayed a positive, upward prediction line most representative of the overall positive trend in the observable time series. All code and images were done in R.

## 6: Multi-Class Image Classification
For this project, Landis and his team conducted multi-class image classification on a dataset of rice grain images. The grains belonged to four distinct rice types; multiple feature extraction methods and classification algorithms were tasked with accurately classifying images in the test sample, after being trained on a training sample from the original dataset. Different method-algorithm pairings were used on the images to compare the effectiveness of the classification across many different parameters and arguments. After analysis, the most accurate pairing was the Histogram of Oriented Gradients (HOG) feature extraction run through Support Vector Machine (SVM) classification, with a score of 0.98 across all evaluation metrics. All code was done in Python with the Jupyter Notebook environment.

## 7: Student Academic Performance Investigative Analysis
Landis and his partner, Eric, wanted to investigate the relationship between a student and their academic performance, based on personal factors. The data was in an unusable format and needed considerable data cleaning; NumPy and Pandas libraries in Python were utilized for this, with the pyplot library used for visualizations. Our data analysis revealed that students with good academic behaviors had higher scores, contrasting with students who did not utilize their free time for school work, yielding lower scores overall. Decision Tree and Random Forest algorithms were used for classification but yielded modest results, suggesting an issue with predicting human behavior without complex methods. All code and visualizations were created in Jupyter Notebook with Python.